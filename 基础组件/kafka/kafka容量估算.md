
kafka容量评估需求场景分析

拿电商平台为例, kafka 集群每天需要承载10亿+请求流量数据，一天24小时，对于平台来说，晚上12点到凌晨8点这8个小时几乎没多少数据涌入的。这里我们使用「二八法则」来进行预估，也就是80%的数据（8亿）会在剩余的16个小时涌入，且8亿中的80%的数据（约6.4亿）会在这16个小时的20%时间 （约3小时）涌入。

QPS计算公式 = 640000000 ÷ (3 * 60 * 60) = 6万，也就是说高峰期集群需要抗住每秒6万的并发请求。

假设每条数据平均按20kb(生产端有数据汇总)来算, 那就是 1000000000 * 20kb = 18T,  一般情况下我们都会设置3个副本, 即54T, 另外 kafka 数据是有保留时间周期的,  一般情况是保留最近3天的数据, 即 54T * 3 = 162T

要搞定10亿+请求,高峰期要支撑6万QPS, 需要大约162T的存储空间


## kafka容量评估之物理机数量

 在第一步中我们分析得出系统高峰期的时候要支撑6万QPS, 如果公司资金和资源充足的情况下, 我们一般会让高峰期的QPS控制在集群总承载QPS能力的30%左右, 这样的话可以得出集群能承载的总QPS能力约为20万左右, 这样系统才会是安全的。



场景总结:

根据经验可以得出每台物理机支撑4万QPS是没有问题的, 从QPS角度分析, 我们要支撑10亿+请求,大约需要5台物理机, 考虑到消费者请求, 需要增加约1.5倍机器, 即7台物理机。


## 容量评估之磁盘
据第一二步骤计算结果, 我们需要7台物理机, 一共需要存储162T数据,大约每台机器需要存储23T数据, 根据以往经验一般服务器配置11块硬盘, 这样每块硬盘大约存储2T的数据就可以了, 另外为了服务器性能和稳定性, 我们一般要保留一部分空间, 保守按每块硬盘最大能存储3T数据。

要搞定10亿+请求, 需要7台物理机, 使用普通机械硬盘进行存储, 每台服务器11块硬盘, 每块硬盘存储2T数据

## kafka容量评估之内存

一个 Topic 会对于多个 partition ,一个 partition 会对应多个 segment , 一个 segment 会对应磁盘上4个log文件。假设我们这个平台总共100个 Topic , 那么总共有 100 Topic * 5 partition * 3 副本 = 1500 partition 。对于 partition 来说实际上就是物理机上一个文件目录, .log就是存储数据文件的, 默认情况下一个.log日志文件大小为1G.

如果要保证这1500个 partition 的最新的 .log 文件的数据都在内存中, 这样性能当然是最好的, 需要 1500  * 1G = 1500 G内存, 但是我们没有必要所有的数据都驻留到内存中, 我们只保证25%左右的数据在内存中就可以了, 这样大概需要 1500 * 250M = 1500 * 0.25G = 375G内存, 通过第二步分析结果,我们总共需要7台物理机, 这样的话每台服务器只需要约54G内存, 外加上面分析的JVM的10G, 总共需要64G内存。还要保留一部分内存给操作系统使用,故我们选择128G内存的服务器是非常够用了。


## cpu cores
我们评估需要多少个 CPU Core，主要是看 Kafka 进程里会有多少个线程，线程主要是依托多核CPU来执行的，如果线程特别多，但是 CPU核很少，就会导致CPU负载很高，会导致整体工作线程执行的效率不高,性能也不会好。所以我们要保证CPU Core的充足, 来保障系统的稳定性和性能最优。

要搞定10亿+请求, 需要7台物理机, 每台物理机内存选择128G内存为主,需要16个cpu core(32个性能更好)

## 网卡

根据第一二步分析结果, 高峰期的时候, 每秒会有大约6万请求涌入, 即每台机器约1万请求涌入(60000 / 7), 每秒要接收的数据大小为: 10000 * 20 kb = 184 M/s, 外加上数据副本的同步网络请求, 总共需要 184 * 3 = 552 M/s。

一般情况下,网卡带宽是不会达到上限的,对于千兆网卡, 我们能用的基本在700M左右,  通过上面计算结果, 千兆网卡基本可以满足, 万兆网卡更好。

## kafka容量评估之核心参数
参数说明与建议
broker.id
每个broker都必须设置一个唯一的ID
log.dirs
这个参数非常重要, kafka 所有的数据都是写入到这个目录下的磁盘文件中的,如果说机器上有多块物理硬盘的话,那么可以把多个目录挂载到不同的物理硬盘上,然后这里可以设置多个目录。这样 kafka 可以把数据分散到多块物理硬盘,多个硬盘可以并发写,提升吞吐量。

上面评估我们每台机器会有11块硬盘对应这里的11个目录:

/data0/log,/data1/log,.....,/data10/log

zookeeper.connect	controller是 kafka 中的管理者,对 kafka 的管理其实就相当于对zk的管理,此参数是链接 kafka 底层的zk集群的
listeners
这是broker监听客户端发起请求的端口号
delete.topic.enable
默认为true, 允许删除topic
log.retention.hours
日志保留策略时间配置, kafka 里面的数据是有生命周期的,就是底层日志文件要保留多长时间, 默认为7天(168小时), 一般我们建议3天(72小时), 另外云服务默认也是3天
log.retention.bytes
即如果分区数据量超过这个设置,会自动被清理掉,默认-1不按照这个策略来清理
log.segment.bytes
段文件默认配置1GB，有利于快速回收磁盘空间，重启kafka加载也会加快，相反如果文件过小，则文件数量比较多，kafka 启动 时是单线程扫描目录(log.dirs)下所有数据文件)，文件较多时性能会稍微降低。可通过如下选项配置段文件大小:log.segment.bytes=1073741824

num.network.threads
负责转发请求给实际工作线程的网络请求处理线程的数量,默认为3个, 上面评估高负载情况下可以设置为9个
num.io.threads
控制实际处理请求的线程数量,默认为8个,上面评估高负载情况下可以设置为32个
message.max.bytes
broker默认接收的消息的最大大小,默认为977kb,建议设置成10M
log.flush.interval.message
log数据文件刷盘策略, 为了大幅度提高 producer写入吞吐量，需要定期批量写文件 优化建议为：每当producer写入10000条消息时，刷数据到磁盘:log.flush.interval.messages=10000

log.flush.interval.ms
log数据文件刷盘策略, 为了大幅度提高 producer写入吞吐量，需要定期批量写文件 优化建议为：每当producer写入10000条消息时，刷数据到磁盘。

 每间隔1秒钟时间，刷数据到磁盘: log.flush.interval.ms=1000

replica.lag.time.max.ms	Follower 副本能够落后 Leader 副本的最长时间间隔, 当前默认值为10秒, 也就是说, 只要一个Follower 副本落后 Leader 副本的时间不连续超过10秒, Kafka 就认为两者是同步的, 即使 Follower 副本中保持的消息要少于 Leader 副本中的消息。
request.required.acks
参考<kafka三高架构设计剖析>中的ack机制篇章



## 总结

请求量 10亿+读写请求
QPS： 高峰期需要支撑6万QPS
存储空间：162T
物理机：7台
硬盘选择：使用普通机械硬盘，硬盘数量每台服务器11块盘, 每块盘2T数据
物理机内存：64G勉强可以支撑,128G性能更佳
CPU Core：16核(32核性能更佳)
网卡：千兆基本可以满足,万兆性能更佳
