那些年我们追过的cpu异常

## 概述
在日常开发中，我们经常遇到和cpu相关的问题，例如： cpu利用率飙升，导致服务响应耗时变长，甚至拒绝请求；cpu利率用接近100%是否正常；cpu的利用率是否能超过100%；合理的cpu利用率是多少，cpu利用率太低有什么问题。

遇到cpu利用率异常的问题，当我们查网上资料时，往往都是一些只言片语，不够准确也不成体系，本文将详细的介绍cpu相关知识和解决思路，理论结合实际，让大家彻底掌握cpu异常处理方式。

要想彻底搞清楚这个问题，我们将从一下几个方面讨论:

1. cpu的工作原理
2. cpu的利用率统计原理
3. cpu和内存
4. cpu与磁盘io
5. cpu异常排查思路
7. 真实线上案例分享

## cpu的工作原理


当讨论cpu利用率时，我们到底在讨论什么？

谈cpu利用率之前，我们先要弄清楚cpu是如何工作的，换句话说，就是cpu如何识别并执行代码的。

我们编写的代码经过编译器编译之后变成汇编代码，汇编代码根据cpu的架构翻译成机器码，机器码就是cpu能执行的指令集。

cpu执行一条指令包括5个阶段:

取指令：通过指令寄存器(PC programe counter)取指令
解码：指令译码器按照预定的指令格式，对取回的指令进行拆解
执行：运行指令
访存取数：根据指令地址码，得到操作数在主存中的地址，并从主存中读取该操作数用于运算。也就是从内存中读取信息加载到CPU参运算
结果写回：执行指令阶段的运行结果数据“写回”到某种存储形式，可能是寄存器，也可能写回到内存。
问题来了，程序编译后的机器码保存在哪里，cpu是如何找到需要执行的指令。

带着问题，我们接着往下看。

程序在启动后，操作系统会分配一段连续的地址空间给应用程序，这段空间被称为【虚拟地址空间】。

虚拟地址空间被分为:

代码文件段，包括可执行代码。
堆段，包括动态分配的内存，从低地址开始向上增长。
栈段，包括局部变量和函数调用的上下文等。
文件映射段，包括动态库、共享内存等，从低地址开始向上增长。
未初始化数据段，包括未初始化的静态变量。
已初始化数据段，包括静态常量；
而程序编译后的机器码就保存在代码文件段中。

如果程序申请的虚拟地址没有加载到内存，那虚拟地址映射实际地址是物理磁盘，所以可以认为，机器码保存在物理磁盘中。

程序在运行时，首先会把程序入口地址保存在pc寄存器中，pc指令保存是当前指令的下一条指令执行地址，换句话说，pc指令告诉cpu要到哪里取下一条指令。

cpu通过pc寄存器找到需要执行的指令，当一条指令执行完成后，pc会更新到下一条执行的指令，这样通过pc不断更新需要执行的指令，实现了代码最终执行。

那虚拟地址和物理地址是如何关联的呢，答案是页表(实际上是多级页表)。

cpu在执行指令访问虚拟地址时，此时虚拟地址如果之前已经别加载到物理内存中，则直接访问，否则会发生中断，进程被挂起，同时操作系统会把把虚拟地址映射的磁盘地址信息加载到内存，然后更新内存页表，之后再唤起进程，使进程变成可运行状态，等待下次调度。

这个过程就是缺页中断。

cpu运行的代码包括用户代码和操作系统的函数(下面简称系统函数)。
当cpu执行用户进程代码时，称为用户态；当cpu运行系统函数时，称为内核态。

用户态到内核态的转变，需要通过系统调用来完成。比如，我们调用open打开文件时，过程是：

现场保存，把当前pc寄存器里的用户态指令保存到当前线程中。
更新寄存器，将pc寄存器需要更新为内核态指令。
cpu跳转到内核态运行内核指令。
当内核指令完成后，cpu寄存器恢复原来保存的用户态指令，然后再切换到用户空间，继续运行进程。
cpu的pc指令的切换称之为上下文切换，除了系统调用可以实现上下文切换以外，还包括进程上下文切换。

系统调用一定会导致上下文切换，但不一定导致进程阻塞，取决于调用的系统函数是否会阻塞进程。

注意，单核cpu同时只能执行一条指令，如果没有执行任何指令，则处于idle状态，此时cpu状态为空闲态。

实际上，cpu处于空闲态时，并不是没有运行任何指令，它一直在运行halt指令，当有可运行的进程后，发出中断指令，这样cpu从空闲态切换到运行态。

如果cpu处于空闲状态，且是因为进程都在等待IO完成(IO操作会导致进程挂起)，这种特殊的空闲态占cpu整个时间的比例，称之为iowait利用率。

因此，当我们说cpu的利用率时，需要关注类型大致可分为：

用户态利用率：cpu执行用户代码的耗时比例
内核态利用率：cpu执行系统函数的耗时比例
空闲态利用率：cpu执行halt的耗时比例
iowait利用率：等待io完成占用的时间比例
当我们判断cpu利用率异常时，需要先考虑是哪种cpu利用率异常。

## cpu利用率统计

cpu利用率统计原理我们清楚了cpu的利用率，那操作系统是如何计算出cpu的利用率的呢？

首先，我们清楚几个知识点:
1. 一台机器可以安装多个cpu
2. 每个cpu可以有多个核，一个核代表一个逻辑处理器。
3. 如果开启了超线程技术，一个核可以分为两个逻辑处理器，这样单核cpu可以实现线程级的并行运行。
通过 cat /proc/cpuinfo 命令，可以查看cpu相关的信息：

```
processor	: 43
vendor_id	: GenuineIntel
cpu family	: 6
model		: 85
model name	: Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz
stepping	: 4
microcode	: 0x200004d
cpu MHz		: 2700.014
cache size	: 16896 KB
physical id	: 1
siblings	: 24
core id		: 9
cpu cores	: 12
apicid		: 51
initial apicid	: 51
fpu		: yes
fpu_exception	: yes
cpuid level	: 22
wp		: yes
clflush size	: 64
cache_alignment	: 64
address sizes	: 46 bits physical, 48 bits virtual
power management:
```

1. physical id: 指的是物理封装的处理器的id。
2. cpu cores: 位于相同物理封装的处理器中的内核数量。
3. core id: 每个内核的 id。
4. siblings: 位于相同物理封装的处理器中的逻辑处理器的数量。
5. processor: 逻辑处理器的 id。

常用命令：

查看有几个cpu: cat /proc/cpuinfo | grep "physical id" | sort -u
查看cpu核数: cat /proc/cpuinfo | grep "cpu cores" | sort -u
查看逻辑处理器数: cat /proc/cpuinfo | grep "processor" | sort -u

如果逻辑处理器数是cpu核数的2倍，说明开启了超线程。

cpu的一个processor代表一个逻辑处理器，cpu有多少个processor就支持多少个程序并行运行。

当跑满cpu的一个processor后，进程的cpu利用率会达到100%，如果跑满拥有24个processor后，理论上利用率可达2400%。

所以，当cpu利用率接近100%，甚至超过100%时，也可能是正常情况。

cpu执行的时间单位是jiffies，比如cpu执行花了100jiffies，处于用户态时间占了30jiffies，那用户态的利用率就是30%，内核态占了40jiffies，那内核态利用率就是40%。

统计cpu利用率的关键就是知道从开机先现在，cpu一共运行了多少jiffies，内核态，用户态，空闲分别占用多少。

linux是文件系统，所有的信息就记录在文件中，cpu信息也不例外。
在linux系统中，cpu的运行时间记录在/proc/stat文件中，内容如下：

```

cat /proc/stat
cpu  554274642 89997229 203187628 19849276830 35027665 0 5289238 49782305 0 0
cpu0 104240490 3283665 18367519 2505417145 8828236 0 2054081 4426830 0 0
cpu1 40025490 17650330 30434059 2440255546 2888130 0 380050 8321213 0 0
cpu2 81968365 12774456 23054831 2443066956 6658615 0 613896 7748107 0 0
cpu3 56534842 11257018 28048035 2549387799 2735090 0 455088 3852975 0 0
cpu4 87634339 10325563 21312108 2421595325 4149923 0 551230 8323024 0 0
cpu5 61476991 9222553 28981025 2510965426 2603010 0 426249 4896234 0 0
cpu6 77338242 11794301 24396572 2428504157 3367223 0 491018 8312620 0 0
cpu7 45055880 13689339 28593476 2550084472 3797433 0 317622 3901299 0 0
从cpu0到cpu7，说明此台机器有8个proccesor，这些数字的单位是jiffies。

```

cpu 554274642 89997229 203187628 19849276830 35027665 0 5289238 49782305 0 0，从第一列到最后一列分别表示:

1. 用户态执行时间(554274642) 从系统启动开始累积到当前时刻，处于用户态的运行时间。
2. nice(89997229) 从系统启动开始累积到当前时刻，nice 值为负的进程所占用的 CPU 时间。
3. 内核态执行时间(203187628) 从系统启动开始累积到当前时刻，处于核心态的运行时间。
4. 空闲态执行时间(19849276830)从系统启动开始累积到当前时刻，不包括IO 等待时间以外的其他等待时间。
5. iowait(35027665) 从系统启动开始累积到当前时刻，等待IO占用的时间。
6~10. 分别是硬中断，软中断，steal，虚拟cpu时间

以用户态利用率为例，在时间点T1到时间点T2这段时间内，cpu-N利用率的计算公式：

1. cpu-N总执行时间 = cpu-N执行时间T2 – cpu-N执行时间T1;
2. 用户态占用时间 = cpu-N用户态时间T2 - cpu-N用户态时间T1;
3. 用户态利用率 = 用户态占用时间 / cpu总执行时间;

其他状态的利用率类似。

除了整机的cpu利用率，我们还需要考虑进程，线程的cpu利用率，实际生产环境中，大部分问题都是某个进程的cpu异常。

为了搞清楚进程，线程的cpu利用率，我们先梳理cpu，操作系统，进程，线程之间的关系。

首先，操作系统就像管理者，负责管理任务，然后把任务分配给进程，任务由进程完成，进程收到分配的任务后，他创建线程，把任务指派给线程。

进程和线程都有一个编号，如果进程只有一个线程，那么进程号和线程号是一样的。

cpu就像工厂车间流水线，线程就是车间工人，线程要完成任务就得进入流水线工作。每个线程都会分配一个cpu，线程工作一段时间后(或者遇到特殊问题，例如io阻塞)就会被叫出cpu，换另一个线程进来工作，这个过程就是线程调度。

把一个进程所有的线程占用cpu的时间汇总在一起，就是进程占用cpu的时间。进程占用cpu的情况记录在文件/proc/$PID/stat中，线程占用cpu的情况记录在/proc/$PID/task/$TID/stat中。

通过这三个文件记录的信息，我们可以计算出进程，线程cpu的利用率。某个进程的cpu文件类似：

```
cat /proc/1345/stat

1345 (appengine) S 1 850 16179 0 -1 1077960704 2276373 6704557630 1419694 391949 556572 293882 6216116 7822405 20 0 25 0 131248381 963592192 2505 18446744073709551615 4194304 7223872 140733741830528 140733741829920 4556963 0 0 1 2143420158 18446744073709551615 0 0 17 4 0 0 0 0 0 9756672 9980032 23887872 140733741839465 140733741839579 140733741839579 140733741846451 0
```

进程cpu字段太多，包括进程总cpu时间，用户态，内核态占用的cpu时间，我们捡几个特别重要的说:

1. pid=1345 进程号
2. name=appengine 进程名
3. state=S 进程状态
4. ppid=850 父进程号
以进程用户态利用率为例，在时间点T1到时间点T2这段时间内，cpu-N利用率的计算公式：

1. 进程cpu-N总执行时间 = cpu-N执行时间T2 – cpu-N执行时间T1;
2. 进程用户态占用时间 = cpu-N用户态时间T2 - cpu-N用户态时间T1;
3. 进程用户态利用率 = 进程用户态占用时间 / 进程cpu总执行时间;

到此，我们应该从原理上理解了cpu利用率，文章后续会推荐相关工具直接查看相关cpu利用率指标。

总结一下，cpu利用率有整机利用率和进程，线程级别利用率，主要关注是用户态，内核态，和空闲态的利用率。

用户代码影响用户态利用率，内核代码影响内核态和空闲态利用率(导致进程挂起都是系统函数)。

换句话说，优化程序性能，就是提升用户态代码执行的效率，减少内核态函数的调用(内核态函数基本无法优化)。

任何语言的优化无外乎都是这两个目标，万变不离其宗，理解这两个目标，代码优化这块也能轻松掌握。


## cpu利用率和内存

我们先问一个问题，内存如何影响cpu利用率？

内存不能改变用户代码，也不能影响改变系统调用，那内存如何影响cpu利用率的呢？要理解这个问题，还得回到cpu的执行原理上。

进程在运行的过程中申请内存，在堆上分配，上文我们提到堆属于虚拟内存空间，也就是说操作系统分配给进程的内存实际上是虚拟内存空间。

为了验证这个结论，我们可以先看一段代码：

```
import "fmt"

func main() {
    // int占用8个字节，make申请了80M内存
	all := make([]int, 1024*10)
	fmt.Println(all[0])

	// 保证进程常驻
	for {
	}
}
```

查看进程
```
ps aux | grep main.go：

USER       PID %CPU   %MEM   VSZ  RSS  TTY    STAT  START   TIME COMMAND
work     13849  7.8  0.0    1148160 21828 pts/8   Sl+  17:00   0:00 go run main.go
```

可以看到进程实际的内存为21828KB，并没有达到80M，同时虚拟内存1148160KB超过了80M，说明申请的是虚拟内存。

我们在回顾一下虚拟内存知识点，操作系统把虚存内存分成若干个大小相等的分区，这样的分区叫做虚拟页。

虚拟页有三种状态：1.未分配(Unallocated）2.未缓存（Uncached）和已缓存（Cached）。
未分配：没有被进程申请的虚拟内存页。
未缓存：已经被申请的虚拟内存页，但还没有加载到内存中。
已缓存：虚拟内存映射的磁盘地址已经加载到内存中。

当用户进程启动时，操作系统就会给进程分配一段独立的虚拟地址空间，当进程调用内存分配函数时，系统就会在进程的虚拟地址空间分配一段空间，这部分空间就是已分配，未缓存。

当用户程序访问未被缓存的虚拟页时，硬件就会触发缺页中断（Page Fault，PF），操作系统需要将磁盘上未被缓存的虚拟页加载到物理内存中，同时更新页表(page table)。

页表中保存了虚拟内存到物理内存的映射关系，通过查找页表，就可以实现虚拟地址到物理地址的转换。申请虚拟内存，缺页中断操作都是通过系统调用实现，也就是说这部分的操作会增加cpu内核态的使用率。

所以，我们要尽可能避免多次申请内存，在golang编程有一个原则：申请容器时需要指定容器的大小。这样可以避免容器空间不足导致多次申请内存，从而尽可能减少系统调用。

第二知识点，物理内存是有限的，当物理内存不足时，操作系统会从选择合适的物理内存页驱逐回磁盘(flush page)，为新的内存页让出位置，选择待驱逐页的过程在操作系统中叫做页面替换(Swapping)。

当进程访问到已经被驱逐到磁盘的内存时，又会发生缺页中断。所以，当内存不足时，会频繁的发生缺页中断，这部分逻辑会增加cpu的内核利用率。

第三个知识点，当出现缺页中断时，进程会被挂起，等待磁盘信息加载到内存中后(相对于读取内存，读取磁盘会非常耗时)，然后进程才会被再次执行，所以，当内存不足时，可能会导致进程cpu用户态降低(等待io就绪)，但是内核态利用率和iowait利用率会升高。

总结一下，当内存不足时，会导致进程内核态利用率升高，同时可能需要从磁盘中加载数据到内存，这个过程很耗时，会影响进程服务的响应时间。

当进程的响应时间变长时，单位时间内处理的请求数变少，导致累计的请求数变大，加剧了内存的不足情况，这样会导致恶性循环，最终可能导致操作系统主动kill进程，就是进程OOM（Out of memory）的情况。

进程发生OOM时，可能会把进程必要内存信息写入到磁盘，此时也会消耗cpu内核态时间，同时io相关指标也会随之改变。

目前大部分服务在容器中运行，一台机器可能部署多个服务，如果多个服务同时发生OOM，那对磁盘IO的影响也会很明显

由此可见，当进程cpu利用率异常时，观察内存是否够用非常有必要。


## cpu利用率与磁盘IO

上文我们提到，cpu访问数据时，如果数据不存在内存中，需要从磁盘中把数据找到并加载到内存。

直观上，磁盘的IO性能直接影响cpu的iowait的利用率，但实际上，如果IO性能低下，当进程的响应时间变长时，单位时间内处理的请求数变少，导致累计的请求数变大，导致内存消耗殆尽，最终影响了cpu的各个方面（上文提到内存对cpu的影响）。

那具体有哪些指标衡量磁盘IO的性能，老规矩，我们先把原理讲清楚，再来分析指标。我们先看下操作系统是如何操作磁盘的。

操作系统发起一次磁盘操作会经过：
1. 虚拟文件层(Virtual File System)
2. 具体文件系统(例如Ext2)
3. Page Cache层 (Page Cache）
4. 通用块层（Generic Block Layer
5. I/O调度层（I/O Scheduler Layer
6. 块设备驱动层（Block Device Driver Layer
7. 物理块设备层（Block Device Layer）

虚拟文件层：是Linux内核的子系统之一，它为用户程序提供文件和文件系统操作的统一接口，屏蔽不同文件系统的差异和操作细节。借助VFS可以直接使用open、read、write这样的系统调用操作文件，而无须考虑具体的文件系统和实际的存储介质。

具体文件系统层：Linux支持了很多种类的文件系统，包含本地文件系统ext3、ext4等。

Page Cache层：Cache层在内存中缓存了磁盘上的部分数据，主要是为了提高Linux操作系统对磁盘访问的性能。
当应用程序调用read读文件时，如果文件内容存在Page Cache中，则直接读取。当应用程序调用write写文件时，如果page Cache有空间，则将数据存在Cache里，然后统一异步写到磁盘中。

通过free -m才可以查看Page Cache层内存数据:
```
git:(master) ✗ free -g
             total       used       free     shared    buffers     cached
Mem:            23         19          4          0          1         11
-/+ buffers/cache:          7         16
Swap:            0          0          0
```


1. cached: 表示当前的页缓存（Page Cache）占用量。
2. buffers: 表示当前块设备(Block Cache) 占用量，这部分内存在块设备驱动层使用，主要是设计用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用。
3. -/+ buffers/cache: 第一个数等于used-buffers-cached， 第二个数据等于free+buffers+cached。
4. Swap: 当物理内存不够用的时候，将部分内存上的数据交换到swap空间上，当我们发现Swap数据使用过大时，可以怀疑内存不足问题。

如果一个文件的页加载到了Page Cache，那么同时buffer cache只需要维护块指向页的指针就可以，不需要重新加载内容到Buffer Cache。
因此，我们现在提起 Page Cache，基本上都同时指 Page Cache 和 buffer cache 两者，直接统称为 Page cached。


通用块层：接收上层发出的磁盘请求，并最终发出I/O请求，该层隐藏了底层硬件块设备的特性，为块设备提供了一个通用的抽象视图。
I/O调度层：当调度层接收通用块层发出的IO请求后，会尝试缓存请求并试图合并相邻的请求，以节省块设备IO操作的次数。


常见的I/O调度算法包括Noop调度算法（No Operation）、CFQ（完全公正排队I/O调度算法）、DeadLine（截止时间调度算法）、AS预测调度算法等。
Noop算法：最简单的I/O调度算法。该算法仅适当合并用户请求，并不排序请求。新的请求通常被插在调度队列的开头或末尾，下一个要处理的请求总是队列中的第一个请求。这种算法是为不需要寻道的块设备设计的。
CFQ算法：算法的主要目标是在触发I/O请求的所有进程中确保磁盘I/O带宽的公平分配。
Deadline算法：算法引入了两个排队队列分别包含读请求和写请求，两个最后期限队列包含相同的读和写请求。

在实际问题处理中，我们不需要了解具体算法，但是我们要建立一个概念：操作系统处理IO请求，不是来一个请求处理一个请求，而是以队列的方式处理IO请求。

也就是说，IO队列的长度一定程度上可以反应磁盘IO的性能。

块设备驱动层和物理块设备层不会影响对cpu利用率的理解，本文不表。

以上都是理论知识，回到具体工作中，假如应用程序调用read命令读取文件，如果page cache中并没有缓存，这会导致一次磁盘读取操作。

而磁盘读取操作相对耗时长，所以从进程整个生命周期来讲，等待IO的时间远大于用户代码的执行时间，这样会导致进程cpu用户态利用率降低，系统态利用率也可能降低，iowait态利用率升高，服务响应耗时增加。

有些业务场景，不管怎么都无法让cpu的整体利用率变大，可能就是服务存在IO问题。


## cpu异常排查思路

至此，我们理解了cpu运行原理，以及内存和磁盘IO对cpu的影响。当cpu异常时，可能出现的故障如下：

1. 应用服务响应时间过长。
2. 处理并发请求能力降低，甚至拒绝请求，响应5xx错误。
3. rpc请求响应时间大于实际配置时间 (进程被挂起)。
4. http请求此服务失败，连接出现5xx错误。
5. tcp请求此服务失败，出现connection failed。
6. 进程的错误日志变多。


也就是说，线上这类问题出现时，需要考虑cpu异常问题，当cpu异常时，排查的思路：

1. 机器性能问题，具体是机器cpu性能太差，内存不足，磁盘IO性能不足。
2. 进程性能问题

### 整机cpu异常排查

通过top命令可以查看机器cpu整体使用情况：
```
> top
20:00:21 up 337 days,  7:53,  0 users,  load average: 0.68, 0.51, 0.45
Tasks: 464 total,   1 running, 463 sleeping,   0 stopped,   0 zombie
top - 20:00:28 up 337 days,  7:54,  0 users,  load average: 0.62, 0.50, 0.45
Tasks: 466 total,   1 running, 464 sleeping,   0 stopped,   1 zombie
Cpu(s):  2.8%us,  1.0%sy,  0.4%ni, 95.4%id,  0.2%wa,  0.0%hi,  0.0%si,  0.2%st
Mem:  24523920k total, 19437376k used,  5086544k free,  1115388k buffers
Swap:        0k total,        0k used,        0k free, 11752020k cached
```


默认情况下，Cpu(s)行只显示了一个cpu processor的利用率，如果需要查看所有逻辑核的利用率，按[1]即可。

top命令输出中，Cpu(s)行解释如下：

us(user cpu time): 用户态利用率
sy(system cpu time): 内核态利用率
id(idle cpu time): 空闲态利用率
wa(io wait cpu time): IO等待利用率，是空闲态的一种，如果此值过大，则可判断此机器有IO问题。
hi (hardware interrupter)：硬中断消耗cpu
si（sortware interupter）：软中断消耗cpu

top命令其他使用经验：

按[P]可以按照cpu的利用率倒序排列
按[M]可以按照mem的利用率倒序排列
按[c]可以查看命令的全路径，这个操作对查看进程使用哪个配置文件特别有用。


2-8原则的使用，在公司一般都有机器性能监控系统，系统会把多核cpu利用率聚合成某个0%到100%的值，如果发现机器的cpu idle% < 20%， us%+sys% > 80%，则可以判断cpu性能不足。

解决此类问题，一般是摘掉此机器的流量，或者是把该机器上服务迁移一部分。


### 内存排查

通过 free -g命令可以查看机器的cpu整体使用情况，内存单位是G。

```

➜ git:(master) ✗ free -g
             total       used       free     shared    buffers     cached
Mem:            23         19          4          0          1         11
-/+ buffers/cache:          7         16
Swap:            0          0          0

```

可用的内存=free+buffers+cached=4+1+11=16G， 如果内存不足，可能会导致系统kill掉某个进程，查看命令dmesg。

```
> dmesg | grep kill
[11686.043641] Out of memory: Kill process 2603 (flasherav) score 761 or sacrifice child
```

如果OOM过多也能作为内存不足的证据：dmesg | grep kill | wc -l


### 磁盘IO排查

通过iostat -x -k命令可以查看磁盘IO性能问题：
```


> iostat -x 3
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           2.77    0.43    1.00    0.16    0.23   95.41

Device:         rrqm/s   wrqm/s     r/s    w/s   rkB/s   wkB/s  rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
vda               0.00     6.75    2.49    5.22  47.25   71.33  94.50   142.65    30.75     0.12   16.12   1.05   0.81
vdb               0.00     2.28    0.46    1.22  5.67    51.62   11.35   103.24    68.23     0.09   52.73   3.03   0.51

```


输出解析如下：
Device： 物理磁盘。
rrqm(merge)：合并发送给磁盘的读请求次数，上文我们提到操作系统为了减少读写磁盘次数，会合并请求。
wrqm(merge)：合并发送给磁盘的写请求次数。
r/s: 每秒读次数。
w/s：每秒写次数。
rkB/s：每秒读的kb数。
wkB/s：每秒写的kb数。
rsec/s(sectors)：读扇区数。
wsec/s(sectors)：写扇区数。
avgrq-sz/s：平均每次IO操作的扇区数。
avgqu-sz：平均队列的长度。
await: 平均每次I/O操作的等待时间 (毫秒)。
svctm: 平均每次I/O操作的服务时间 (毫秒)。
%util: IO队列中不为空的时候占总时间的比例，通常用来衡量磁盘的繁忙度。

首先我们需要清楚服务部署在哪个磁盘，也就是看服务部署的目录是挂载在哪块磁盘上，其次，我们主要关注avgqu-sz，await，%util这三个指标，来判断磁盘性能。

网上很多文章对着三个指标都有一些错误的理解，比如%util接近100%就一定认为磁盘IO有问题，实际上这样的理解是非常片面的。

先看avgqu-sz这个指标。

avgqu-sz表示队列的长度，大家或许问avgqu-sz多少才是一个合适的值，这个值没有定数，不同的磁盘这个值的临界值不一样，但如果存在性能问题，这个值一定是连续增长(比正常时刻大很多)，所以我们需要用线性变化的思维来分析磁盘性能问题。

如果上游请求数没有明显增加，但是avgqu-sz明显增加，说明磁盘IO可能存在问题，导致累计到队列的IO请求数越来越多。

await的时间包括了等待在队列里面的时间和执行IO时间，如果这个时间明显增加，则表示磁盘IO可能存在问题。

同时我们注意，在cpu利用率里面有个iowait%的指标，iowait是进程在等待IO准备好时，处于挂起状态，cpu没有可运行的进程而处于空闲态。

换句话说，只有cpu在空闲态，且空闲态是因为等待IO导致的，才会导致这个值增加，因此这个值特别高，可能存在磁盘IO问题。

%util指IO队列中不为空的时候占总时间的比例，它不关心IO队列里面等待IO请求的个数，只关心队列里有没有请求。

如果一个IO请求耗时0.1s中，那1s中可以处理10个请求，如果上游依次提交10个请求，磁盘队列在1s内都有请求，此时%util接近100%。

但是对于有并发能力的磁盘来说，比如ssd类型磁盘并发可以处理10个请求，因为上面是依次提交了10个请求，导致磁盘队列内都有请求，此时它的util也是100%。
但是，实际上这个磁盘并没有性能瓶颈，如果上游并发的提交10个请求，此磁盘依然有能力处理。

所以，%util可以判断磁盘IO性能的依据，但是也需要考虑磁盘是否具有并发性。

通过cat /sys/dev/block/*/queue/rotational 命令可以查看磁盘是否是ssd磁盘，输出0表示是ssd磁盘，否则不是。

总结一下，如果发现avgqu-sz，await，%util这三个指标都有明显突增，则表示磁盘大概率存在性能问题。

### 进程cpu异常排查

下面我们讨论一下某个具体进程cpu异常问题，如果进程出现响应时间明显突增等错误，而业务日志中没有明显的代码逻辑错误，则有可能是进程cpu异常导致，处理思路:

1. 定位问题发生的时间段，确定有问题的机器，重中之重。
2. 查看这段时间内，进程的运行日志，重点关注qps，错误数，响应耗时，rpc请求耗时，rpc请求重试等信息。
3. 查看这段时间内，进程使用的cpu，内存信息
4

遇到这种情况，可以使用pidstat命令查看进程cpu信息:
```
$ pidstat  -p 9880 -u -l 1
Linux 4.14.0_1-0-0-27 	09/15/2021 	_x86_64_	(48 CPU)

05:41:18 PM       PID    %usr %system  %guest    %CPU   CPU  Command
05:41:19 PM      9880   53.00    8.00    0.00   61.00     1  ./bin/ae-app-337
05:41:20 PM      9880   55.00    9.00    0.00   64.00     1  ./bin/ae-app-337
05:41:21 PM      9880   53.00    9.00    0.00   62.00     1  ./bin/ae-app-337
05:41:22 PM      9880   52.00   10.00    0.00   62.00     1  ./bin/ae-app-337
05:41:23 PM      9880   56.00    9.00    0.00   65.00     1  ./bin/ae-app-337
05:41:24 PM      9880   49.00   10.00    0.00   59.00     1  ./bin/ae-app-337
05:41:25 PM      9880   63.00    9.00    0.00   72.00     1  ./bin/ae-app-337
```

pidstat 可以查看指定进程的\%usr, \%system使用率，除此之外，还可以查看进程内存，i/o信息：

1. -r: 查看进程的内存信息
2. -d：查看IO统计信息，包括进程每秒读写磁盘数
3. -t: 查看进程所有线程的指定指标

\%CPU的值到底是多少才表示cpu有性能问题呢，和IO一样，也没有准确的值，在监控系统中，如果发现这个值突然飙升，则表示有cpu瓶颈问题。

当确定应用进程有问题时，一般就是代码性能有问题，低性能的代码包括：

1. 代码死循环，超大循环，正则表达式，GC等。
2. 内存分配问题，例如分配不当导致频繁扩容，分配过多用不到的内存，内存泄露。
3. 不合理的系统调用，例如读取超多文件，加锁不合理，死锁。
4. 线程泄露，协程泄露等。
5. 遇到这类通常需要使用特殊工具找到低效代码并进行优化，对于golang语言来讲，我们使用pprof工具，其他语言也有类似工具。

### vmstat使用

```
vmstat 2
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  0      0 16963800   3440 8607132    0    0     5     8    8    4  2  0 98  0  0
 0  0      0 16964188   3440 8607132    0    0     0     4 2601 2123  0  0 100  0  0
 0  0      0 16964372   3440 8607132    0    0     0     2 3695 3146  1  0 99  0  0
```

核心指标如下：
1. r (running): 可运行的进程数
2. b (blocked): 阻塞的进程数，过多说明磁盘io可能有问题
3. in (interrupt number): 中断的次数，次数越多说明cpu不停的需要请求资源
4. cs(contenxt switch): 上下文切换的次数，过多可能是进程数量过多，或者io有问题
5. us(user cpu time): 用户进程的cpu利用率
6. sy(system cpu time): 内核进程的cpu利用率
7. wa(io wait): 当可运行的进程因为等待io而处于blokced状态，这个时间所占的比例
8. id(idle): cpu完全空闲的时间的比例


### strace使用

除了pprof工具以外，另外一个工具就是strace，用于定位cpu内核态利用率异常问题。

strace命令跟踪进程的所有的系统调用和所接受的信号，如果strace执行不正常，可以切换到root的权限运行。

strace使用并不难，难的是我们能理解strace的输出，并从输出结果中分析出问题。

可以使用strace的场景：

1. 进程内核态利用率过高
2. 进程启动不正常，比如服务启动失败等
3. 命令执行失败，比如wget，git，cp等任何命令执行失败，又没有比较合适的工具排查时，都可以使用strace命令。

strace命令输出非常复杂，看懂strace命令的输出是使用的关键，使用方式：strace command。

strace输出每一行都是一条系统调用，等号左边是系统调用的函数名及其参数，右边是该调用的返回值，例如读取文件的strace部分输出内容:

```
openat(AT_FDCWD, "main.go", O_RDONLY|O_CLOEXEC) = 3
read(3, "package main\n\nimport (\n\t\"fmt\"\n\t\""..., 512) = 222
```

解释如下：

1. openat是系统调用函数
2. AT_FDCWD函数调用参数，表示打开相对于当前目录的文件
3. main.go函数调用参数，表示需要操作的文件
4. O_RDONLY|O_CLOEXEC函数调用参数，表示只读的方式开发文件，并且文操作结束后，关闭文件。
5. 3是函数返回值，表示文件句柄的值。
6. read(3, content...) = 222表示从fd=3的文件读取内容，222表示字节数。

看懂strace输出的前提是你需要了解常用的系统调用知识，常见的系统调用有:

1. 文件操作相关，例如open, pennat, close, read, readv, pread, write, stat, fstat, lstat等
2. i/o复用相关，例如poll, select,epoll_create,epoll_wait，eventfd
3. 网络操作相关，例如socket,bind, accept, listen, connect,read,send等
4. 锁操作相关，例如futex，输出为futex(0x559258, FUTEX_WAKE_PRIVATE, 1) = 1 其中 FUTEX_WAKE_PRIVATE是标识位，表示唤醒阻塞的进程，当标志位是FUTEX_WAKE_PRIVATE时，返回值1表示唤醒的进程数量，常用的还有FUTEX_WAIT表示阻塞当前进程，直到为唤起，返回值0表示当前进程被FUTEX_WAKE唤起。
5. 内存操作，例如mmap用于把硬盘上的文件与内存之间建立映射，也就是把虚拟内存和物理内存建立映射表。

遗憾的是，看懂了strace的输出内容后，依然很难定位问题，我们需要一些技巧分析strace的日志。


使用strace -c 统计系统调用的次数和耗时， demo程序如下:

```

package main

import (
	"fmt"
	"io/ioutil"
	"os"
)

func main() {
	file, err := os.Open("file")
	if err != nil {
		panic(err)
	}
	defer file.Close()
	_, err = ioutil.ReadAll(file)

	fmt.Println(err)
}
```

当file的大小是1M左右时，执行结果:

```

> time ./main
./main 0.00s user 0.01s system 106% cpu 0.005 total
strace的日志如下：

strace -c ./main
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 55.04    0.002129          19       114           rt_sigaction
 10.94    0.000423          24        18           mmap
  8.22    0.000318         106         3           clone
  5.69    0.000220          28         8           rt_sigprocmask
  2.66    0.000103          52         2         1 openat
  2.35    0.000091          46         2           futex
  2.25    0.000087          29         3           fcntl
  2.04    0.000079          79         1           close
  1.47    0.000057          57         1           readlinkat
  1.22    0.000047          47         1           write
  1.22    0.000047          16         3         2 epoll_ctl
  1.16    0.000045          45         1           execve
  1.01    0.000039          13         3           rt_sigreturn
  0.72    0.000028          14         2           read
  0.72    0.000028          28         1           pipe2
  0.67    0.000026          13         2           sigaltstack
  0.65    0.000025          25         1           arch_prctl
  0.59    0.000023          23         1           uname
  0.57    0.000022          22         1           epoll_create1
  0.47    0.000018          18         1           sched_getaffinity
  0.34    0.000013          13         1           gettid
------ ----------- ----------- --------- --------- ----------------
100.00    0.003868                   170         3 total
当file的大小是2G左右时， 执行结果:

> time ./main
./main  1.43s user 5.68s system 143% cpu 4.973 total
strace的日志如下：

strace -c ./main
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 99.20    3.007300       34567        87         8 futex
  0.59    0.017791           5      3357         1 rt_sigreturn
  0.14    0.004120          36       114           rt_sigaction
  0.02    0.000665          35        19           mmap
  0.02    0.000562          51        11           read
  0.01    0.000249          31         8           rt_sigprocmask
  0.01    0.000202          67         3           clone
  0.00    0.000143          48         3           sched_yield
  0.00    0.000083          42         2         1 openat
  0.00    0.000069          17         4           epoll_pwait
  0.00    0.000067          34         2           sigaltstack
  0.00    0.000061          31         2           tgkill
  0.00    0.000048          48         1           arch_prctl
  0.00    0.000038          38         1           execve
  0.00    0.000032          32         1           sched_getaffinity
  0.00    0.000030          30         1           gettid
  0.00    0.000029          29         1           uname
  0.00    0.000000           0         1           nanosleep
  0.00    0.000000           0         2           getpid
  0.00    0.000000           0         3           fcntl
  0.00    0.000000           0         3         2 epoll_ctl
  0.00    0.000000           0         1           readlinkat
  0.00    0.000000           0         1           epoll_create1
  0.00    0.000000           0         1           pipe2
------ ----------- ----------- --------- --------- ----------------
100.00    3.031489                  3629        12 total

```

可以看到，读取大文件时，内核态的cpu消耗远大于用户态，通过strace可知主要的消耗在futex系统调用上，read的耗时也从0.000028增加到0.000562。

在真实环境中，如果某个实例的内核态利用率异常，我们可以通过strace -c命令分别统计正常实例和异常实例的系统调用，对比发现到异常的系统调用。

同时，我们还可以通过strace -c命令看到openat函数只调用了50次左右，我们可以通过这个值来判断是否是因为读取文件数太多导致系统异常。

曾经，我们真实的线上案例中，就因为定期扫描配置文件过多导致系统间隙性内核态利用率变高，通过查看open, stat这样的系统调用命令就能很快定位出来。

统计系统调用的时间
使用参数如下

-r：相对时间输出
-t：输出中的每一行前加上时间信息，精确到秒
-ttt：微秒级输出,以秒了表示时间
-T：显示每一个系统调用的时间
继续使用读取文件的程序做demo，执行strace -ttt -T ./main 输出结果：

```

1631948704.825352 readlinkat(AT_FDCWD, "/proc/self/exe", "/work/vsgo-179/baidu/ps-aladdin/vsgo/main.bak", 128) = 45 <0.000068>
1631948704.825553 futex(0x559358, FUTEX_WAKE_PRIVATE, 1) = 1 <0.000033>
1631948704.825652 futex(0x559258, FUTEX_WAKE_PRIVATE, 1) = 1 <0.000062>
1631948704.825843 fcntl(0, F_GETFL)     = 0x8002 (flags O_RDWR|O_LARGEFILE) <0.000076>
1631948704.826040 futex(0xc000080548, FUTEX_WAKE_PRIVATE, 1) = 1 <0.000120>
1631948704.826258 fcntl(1, F_GETFL)     = 0x8002 (flags O_RDWR|O_LARGEFILE) <0.000046>
1631948704.826393 fcntl(2, F_GETFL)     = 0x8002 (flags O_RDWR|O_LARGEFILE) <0.000046>
1631948704.826545 openat(AT_FDCWD, "test", O_RDONLY|O_CLOEXEC) = 3 <0.000057>
1631948704.826696 epoll_create1(O_CLOEXEC) = 4 <0.000049>
1631948704.826815 pipe2([5, 6], O_NONBLOCK|O_CLOEXEC) = 0 <0.000044>
1631948704.826929 epoll_ctl(4, EPOLL_CTL_ADD, 5, {EPOLLIN, {u32=5801392, u64=5801392}}) = 0 <0.000044>
1631948704.827063 epoll_ctl(4, EPOLL_CTL_ADD, 3, {EPOLLIN|EPOLLOUT|EPOLLET|0x2000, {u32=1381375624, u64=139935710846600}}) = -1 EPERM (Operation not permitted) <0.000027>
1631948704.827163 epoll_ctl(4, EPOLL_CTL_DEL, 3, {0, {u32=0, u64=0}}) = -1 EPERM (Operation not permitted) <0.000028>
1631948704.827277 read(3, "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 512) = 512 <0.000043>
1631948704.827414 read(3, "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 1024) = 1024 <0.000044>
1631948704.827562 read(3, "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 2048) = 2048 <0.000070>
1631948704.827796 read(3, "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 4096) = 4096 <0.000056>
1631948704.827959 read(3, "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 8192) = 8192 <0.000059>
1631948704.828121 read(3, "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 16384) = 16384 <0.000061>
1631948704.828315 read(3, "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 32768) = 32768 <0.000073>
1631948704.828523 read(3, "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 65536) = 65536 <0.000083>
1631948704.828719 mmap(NULL, 1439992, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f45523f9000 <0.000061>
1631948704.828942 read(3, "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 131072) = 131072 <0.000095>
1631948704.829240 read(3, "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 262144) = 262144 <0.000158>
1631948704.830227 read(3, "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 524288) = 524288 <0.000193>

```

我们可以清晰的看到每个系统调用的耗时时间和执行时间点。

通过以上分析，我们可以看到在1631948704.825352时间点可以运行进程，在1631948704.826545时间点开始读取文件，在1631948704.830227时间点read完成，等待数据返回给进程。

我们曾经遇到线上问题，nginx请求数据到hhvm，间歇性的发现nignx的请求到hhvm全部超时，大概超时30s，此时内核态cpu特别高。

我们通过strace命令发现请求过来到执行第一个php文件，这段时间就持续了30s左右。

这种问题，我们可以配合grep命令查看php执行的时间：strace -ttt -T hhvm -c conf | grep php --color

而在php执行之前，读取了大量的配置文件，导致服务持续30s不可用，我们通过分析php文件开始执行的时间找到了问题所在。

过滤无关的系统调用，只关心有用的系统调用
使用参数:

-e trace=set
只跟踪指定的系统调用.
例如:-e trace=open,close,read,write, stat
-e trace=file
只跟踪有关文件名操作的系统调用, 只跟踪以文件名作为参数的系统调用，并不包括read, write这种，因为这种函数的参数是fd，而不是文件名
-e trace=process
只跟踪有关进程控制的系统调用.
-e trace=network
跟踪与网络有关的所有系统调用.
跟踪指定pid，结果输出到指定文件 strace -o filename -p pid
-o filename
将strace的输出写入文件filename
-p pid
跟踪指定的进程pid
-f
跟踪由fork调用所产生的子进程.
将strace的结果写入到文件后，我们可以配合grep, awk命令分析出更多我们需要在知道的结果。

例如分析特定时间内打开特定文件的数：cat strace.output | grep "22:20:[0-9]*" | grep "\.conf" | wc -l

类似的还可以看加锁的次数，socket处理的次数等信息。


## 线上案例分析

我们线上追查的cpu高负载主要是内核态负载过高导致服务不可用。



### 定期超大文件扫描导致服务间歇性30s内不可用

现象：通过日志发现某一时间点后突全部php请求都触发30s超时，hhvm报错：Fatal error: entire web request took longer than 30s seconds and timed out。这些数值的单位是
这种问题会持续一段时间，然后慢慢恢复，多个app都出现类似问题。

登陆故障机器，使用pidstat -p pid观察故障实例，发现cpu利用率飙升(超过5个核，正常情况下只有0.7个核左右)，同时cpu利用率约等于system，可以得出，主要是系统调用导致cpu利用率飙升，造成服务不可用。


确定了系统调用导致的问题后，我们使用strace -o output -p pid -T -tt 跟踪指定的进程号得到输出结果。

因为我们怀疑是读取大量配置文件导致的%system飙升，然后服务处理配置文件过长，导致无法处理php正常请求，最终导致服务不可用。

于是我们使用命令cat output | grep "22:23:*" | grep conf排查一分钟内处理配置文件的个数，经过排查多个时间点后，我们发现最高1分钟内居然有40w的conf读取。
我们发现处理配置用的系统调用命令是stat，最长的stat命令高达10ms。

我们使用目录cat output | grep "22:2*:*" | grep php，对比系统调用的时间发现确实直到处理配置文件结束后才慢慢开始php文件。

到目前我们确定了问题所在，直接看代码，发现代码有个两个策略，一个是定时扫描全局配置文件，一个是如果php发现配置文件不存在则扫描全局配置文件。
如果这两个策略同时触发则更加剧服务的cpu负担，同时全局配置文件6w+。

解决的方法也很简单，第一步我们进行配置文件瘦身，把6w的配置文件清理到1k左右，第二步把全量扫描给出最近10min内有变化的配置文件进行加载，第三步修改代码逻辑做到按需加载
最后一步，预加载，因为我们已经实现了按需加载，所以在服务启动时，就预先加载配置文件，避免主线程和加载配置的线程同时触发扫描逻辑。


### 同步日志写入导致服务sys持续升高

现象：服务支持的最大稳定QPS一直无法提升，就算增加服务的可用cpu核数，依然无济于事。
当QPS稳定增加时，cpu利用率升高，但是用户态的cpu基本保持不变，升高的只是内核态的利用率，而且内核态的利用率与整体cpu利用率基本保持一致。

通过这个现象我们可知，服务的整体性能应该受制于系统调用。

我们猜想，可能是日志组件导致性能问题，我们日志组件采用同步写入的方式，也就是直接调用write函数，单线程写入文件时，受制于IO的限制，不管开始多少线程都无法高效利用cpu。
如果多线程同步写入日志，还有加锁，解锁开销(防止日志写乱)，更加剧了系统调用的负担。

我们关闭日志组件后，服务恢复正常，QPS也能被压上去，后续通过改造日志组件解决这个问题。

改造方式：给每个日志线程设计了threadlocal cache缓冲池。后续开启独立进程把cache的数据写回到磁盘。这样避免了请求工作线程阻塞在写日志组件上。


### 混合部署的影响

现象：每天在一段时间内，服务的响应时间突然升高。

排查得知：在故障时间内，机器共有10个app, 平均发生了40次oom, 从而导致cpu负载异常，从而到app响应时间异常。
我们统计了故障时间内和非故障时间内，oom的次数在故障时间内明显变多，同时，每次发生oom后，操作系统都会把进程page cache中写到磁盘中。
这个过程中，由于操作系统会把page cache的数据写回到磁盘，会导致await和io util指标上升。

通过监控对比，内存，cpu，io_await, io_util在故障时间内抖动都保存一致，于是我们迁移了相关服务后，整体服务恢复正常。
