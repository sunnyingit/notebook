IO复用和零拷贝总结

## 概述

Unix统下有5中基本的IO模型：

1. 阻塞式I/O
2. 非阻塞式I/O
3. I/O 复用(select, poll)
4. 信号驱动式I/O
5. 异步I/O

首先需要搞清楚I/O是，应用进程访问系统内核读取数据，涉及到应用进程和系统内核之间的通信。

阻塞式I/O：应用进程访问系统内核，比如使用recvfrom函数，如果此时内核数据没有准备好，则应用进程会阻塞。需要清楚“阻塞”是进程的一种状态，如果数据没有准备好，则进程会由运行态变为挂起状态，让出CPU控制权。

当数据准备好的时候，应用进程会有挂起态变为就绪态，等待CPU调用从而处理准备好的数据。

如果进程阻塞了，这个时候就算有其他fd数据准备好也无法处理，因为应用进程因为I/O阻塞而让出CPU了，所以阻塞式I/O特别低效。

非阻塞式I/O： 应用进程访问系统内核，比如使用recvfrom函数，如果此时内核数据没有准备好， 内核会给应用程序返回错误EWOULDBLOCK，如果应用程序反复调用recvfrom，那么内核将持续返回错误EWOULDBLOCK，直到有数据准备好。

应用程序反复调用recvfrom时，我们称为轮询，这个过程会大量消耗CPU。

IO复用： 所谓IO复用是一种能力，应用程序首先告知内核需要监听套接字，内核如果发现应用程序制定的一个或多个I/O条件准备就绪了，内核就会通知应用程序。

通过select或者poll函数，达到我们所期待的效果。应用程序会阻塞在select或者poll这两个系统调用上，而不是阻塞在真正的I/O调用上，一旦select或者poll发现一个或者多个I/O条件准备就绪，就通知应用程序。

信号驱动式I/O：信号驱动I/O是应用程序首先创建一个I/O信号处理函数，如果I/O条件准备就绪，内核会通过事先注册的信号通知到应用程序,应用程序在整个过程中没有任何阻塞。

异步I/O： I/O操作是异步处理的，当I/O操作完成后内核才会通知应用程序。 内核完成I/O操作的过程是自动的。异步I/O和信号驱动I/O的区别是，信号驱动I/O是数据准备好后，内核通知应用程序调用recvfrom函数处理I/O数据，而异步I/O是自动处理I/O，处理完成后通知应用程序I/O操作已经完成。


## 什么是非阻塞式I/O

首先需要明确，套接字默认是阻塞的(包括监听套接字，已连接的套接字)，这就说明，如果发出一个不能立即完成的套接字调用时，应用进程会阻塞，让出CPU。

可能阻塞套接字调用的操作有：

1. 读操作【输入】(read， readv，recv，recvfrom，recvmsg) 如果应用进程对一个阻塞的TCP套接字发起读操作时，数据没有准备好，则进程挂起。
2. 写操作【输出】(write，writev， send，sendto， sendmsg)如果应用进程对一个阻塞的TCP套接字发起写操作时，如果该套接字发送缓存区没有空间，则进程挂起。
3. 接受外来连接，即调用Accept函数，如果应用进程对一个阻塞的TCP监听套接字发起accept操作时，如果此时没有新的连接达到，则进程挂起。
4. 请求外来连接 即调用Connect函数， 我们知道，只有当三次握手完成后，才算连接建立，所以对一个阻塞的TCP套接字发起Connect请求，应用进程会阻塞直到三次握手完成。

对于非阻塞式套接字，则返回EWOULDBLOCK错误。

## 为什么 I/O 复用要搭配非阻塞 I/O
我们知道 I/O 复用阻塞在select，poll系统调用上，当I/O条件准备就绪，内核告知应用进程数据已经准备好了，此时系统进程去发起返回的套接字调用，比如应用程序通过函数readn获取10字节大小的套接字接受的数据，但是套接字里面只有5字节的数据，所以应用程序还是会挂起。

又比如，套接字接受的数据是10字节，应用程序调用readn只读取了3字节，但应用程序再次读取8字节时，应用程序还是会挂起。

也就是说，应用程序保证不被挂起的状态下读取到所有已经接受的数据。

这个时候必须要求套接字是非阻塞的。



## select，epoll的调用逻辑


select而言：

```
select(int maxfdp1,*readset,*writeset,*exceptset, timeout)
```
1. 可以注册可读，可写，异常的相关套接字
2. timeout取值: None: 一直阻塞知道数据准备好，0: 不阻塞，立即返回，相当于轮询； >0: 阻塞的时间，当超过阻塞的时间还没有数据准备好则返回为空的集合
3. maxfdp1 = sockfd + 1 select在内核中是轮询的，内核遍历所有注册的fd，查看是否有准备好的fd， 套接字是从0开始的，所以maxfd = max(socket) + 1，这也是select的限制(轮询和描述符数量限制)

epoll而言:

1. 没有描述符数量的限制
2. 事件触发机制不需要轮询
3. 内存管理方式更高效(不需要复制fd, 内部使用红黑树管理fd)
4. 给套接字注册相关的事件(可读，可写，异常)

epoll 两种触发模式:

1. LT(level triggered) 是默认/缺省的工作方式，同时支持 block和no_block socket。这种工作方式下，内核会通知你一个fd是否就绪，然后才可以对这个就绪的fd进行I/O操作。就算你没有任何操作，系统还是会继续提示fd已经就绪，不过这种工作方式出错会比较小，传统的select/poll就是这种工作方式的代表。

2. ET(edge-triggered) 是高速工作方式，仅支持no_block socket，这种工作方式下，当fd从未就绪变为就绪时，内核会通知fd已经就绪，并且内核认为你知道该fd已经就绪，不会再次通知了，除非因为某些操作导致fd就绪状态发生变化。如果一直不对这个fd进行I/O操作，导致fd变为未就绪时，内核同样不会发送更多的通知，因为only once。所以这种方式下，出错率比较高，需要增加一些检测程序。

LT可以理解为水平触发，只要有数据可以读，不管怎样都会通知。而ET为边缘触发，只有状态发生变化时才会通知，可以理解为电平变化。

## I/O就绪的条件是什么
LT模式也就是水平触发模式，是epoll的默认触发模式（select和poll只有这种模式） 触发条件：

1. 可读事件：接受缓冲区中的数据大小高于低水位标记，则会触发事件
2. 可写事件：发送缓冲区中的剩余空间大小大于低水位标记，则会触发事件
3. 发生异常
3. 低水位标记：一个基准值，默认是1

ET模式也就是边缘触发模式，如果我们将socket添加到epoll_event描述符的时候使用了EPOLLET标志, epoll就会进入ET工作模式。
触发条件

1. 可读事件：不关心接受缓冲区是否有数据，每当有新数据到来时，才会触发事件。
2. 可写事件：剩余空间从无到有的时候才会触发事件


## IO复用总结

多路复用快的原因在于，操作系统提供了这样的系统调用，使得原来的 while 循环里多次系统调用， 变成了一次系统调用 + 内核层遍历这些文件描述符。

在多路复用IO模型中，会有一个内核线程不断地去轮询多个 socket 的状态，只有当真正读写事件发送时，才真正调用实际的IO读写操作。因为在多路复用IO模型中，只需要使用一个线程就可以管理多个socket，系统不需要建立新的进程或者线程，也不必维护这些线程和进程，并且只有真正有读写事件进行时，才会使用IO资源，所以它大大减少来资源占用。


## 零拷贝

在传统的IO模式中，应用程序从磁盘读取数据然后通过socket套接字发出去需要经过几步：
1. 应用程序调用read函数，发起读取请求,此时cpu的状态会从应用态变成内核态，应用程序切换到阻塞状态，等待数据ready。
2. cpu接收到read指令后向磁盘发起IO请求，通过DMA(direct memory access)技术从磁盘读取数据。
3. 磁盘数据读取完成后，DMA 磁盘控制器会接受到磁盘的通知，将数据从磁盘控制器缓冲区拷贝到内核缓冲区。
4. DMA 磁盘控制器向 CPU 发出数据读完的信号，由 CPU 负责将数据从内核缓冲区拷贝到用户缓冲区。
5. 用户进程由内核态切换回用户态，解除阻塞状态，然后等待 CPU 的下一个执行时间钟。

这个过程中会发生2次上下文切换，1次CPU缓冲拷贝和1次DMA缓冲拷贝。

应用程序再把通过write函数把数据写到socket中，和调用write类似，也会发生2次cpu上下文切换，1次CPU缓冲拷贝和1次DMA缓冲拷贝。

在 Linux 中零拷贝技术主要有 3 个实现思路：用户态直接 I/O、减少数据拷贝次数以及写时复制技术。

1. 用户态直接 I/O：应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输。这种方式依旧存在用户空间和内核空间的上下文切换，硬件上的数据直接拷贝至了用户空间，不经过内核空间。因此，直接 I/O 不存在内核空间缓冲区和用户空间缓冲区之间的数据拷贝。
2. 减少数据拷贝次数：在数据传输过程中，避免数据在用户空间缓冲区和系统内核空间缓冲区之间的CPU拷贝，以及数据在系统内核空间内的CPU拷贝，这也是当前主流零拷贝技术的实现思路。
3. 写时复制技术：写时复制指的是当多个进程共享同一块数据时，如果其中一个进程需要对这份数据进行修改，那么将其拷贝到自己的进程地址空间中，如果只是数据读取操作则不需要进行拷贝操作。


### mmap + write
一种零拷贝方式是使用 mmap + write 代替原来的 read + write 方式，减少了 1 次 CPU 拷贝操作。mmap 是 Linux 提供的一种内存映射文件方法，即将一个进程的地址空间中的一段虚拟地址映射到磁盘文件地址，mmap + write 的伪代码如下：
```
tmp_buf = mmap(file_fd, len);
write(socket_fd, tmp_buf, len);
```

使用 mmap 的目的是将内核中读缓冲区（read buffer）的地址与用户空间的缓冲区（user buffer）进行映射，从而实现内核缓冲区与应用程序内存的共享，省去了将数据从内核读缓冲区（read buffer）拷贝到用户缓冲区（user buffer）的过程，然而内核读缓冲区（read buffer）仍需将数据到内核写缓冲区（socket buffer）。


### sendfile

sendfile 系统调用在 Linux 内核版本 2.1 中被引入，目的是简化通过网络在两个通道之间进行的数据传输过程。sendfile 系统调用的引入，不仅减少了 CPU 拷贝的次数，还减少了上下文切换的次数，它的伪代码如下：
```
sendfile(socket_fd, file_fd, len);
```


通过 sendfile 系统调用，数据可以直接在内核空间内部进行 I/O 传输，从而省去了数据在用户空间和内核空间之间的来回拷贝。与 mmap 内存映射方式不同的是， sendfile 调用中 I/O 数据对用户空间是完全不可见的。也就是说，这是一次完全意义上的数据传输过程。
